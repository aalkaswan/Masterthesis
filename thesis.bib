% This file was created with JabRef 2.3.1.
% Encoding: ISO8859_1
@article{TypeInferenceSurvey,
author = {Caballero, Juan and Lin, Zhiqiang},
title = {Type Inference on Executables},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2896499},
doi = {10.1145/2896499},
journal = {ACM Comput. Surv.},
month = may,
articleno = {65},
numpages = {35},
keywords = {binary code analysis, Type inference, program executables}
}
@INPROCEEDINGS{CATI,
  author={Chen, Ligeng and He, Zhongling and Mao, Bing},
  booktitle={2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)}, 
  title={CATI: Context-Assisted Type Inference from Stripped Binaries}, 
  year={2020},
  volume={},
  number={},
  pages={88-98},
  doi={10.1109/DSN48063.2020.00028}}

@misc{InlinedFunc,
    author={Ahmed, Toufique and Devanbu, Premkumar and Sawant, Anand Ashok},    
    journal={IEEE Transactions on Software Engineering},   
    title={Learning to Find Usage of Library Functions in Optimized Binaries},   
    year={2021},  volume={},  number={},  
    pages={1-1},  
    doi={10.1109/TSE.2021.3106572}}

@misc{roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{allamanis_adverse,
	address = {Athens Greece},
	title = {The adverse effects of code duplication in machine learning models of code},
	isbn = {978-1-4503-6995-4},
	url = {https://dl.acm.org/doi/10.1145/3359591.3359735},
	doi = {10.1145/3359591.3359735},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 2019 {ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {ACM},
	author = {Allamanis, Miltiadis},
	month = oct,
	year = {2019},
	pages = {143--153},
	file = {Ingediende versie:C\:\\Users\\aalkaswan\\Zotero\\storage\\NM33LK4Y\\Allamanis - 2019 - The adverse effects of code duplication in machine.pdf:application/pdf},
}
@inproceedings{recommend_summarization,
    title = "Recommendations for Datasets for Source Code Summarization",
    author = "LeClair, Alexander  and
      McMillan, Collin",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1394",
    doi = "10.18653/v1/N19-1394",
    pages = "3931--3937",
    abstract = "Source Code Summarization is the task of writing short, natural language descriptions of source code. The main use for these descriptions is in software documentation e.g. the one-sentence Java method descriptions in JavaDocs. Code summarization is rapidly becoming a popular research problem, but progress is restrained due to a lack of suitable datasets. In addition, a lack of community standards for creating datasets leads to confusing and unreproducible research results {--} we observe swings in performance of more than 33{\%} due only to changes in dataset design. In this paper, we make recommendations for these standards from experimental results. We release a dataset based on prior work of over 2.1m pairs of Java methods and one sentence method descriptions from over 28k Java projects. We describe the dataset and point out key differences from natural language data, to guide and support future researchers.",
}
@inproceedings{CodeT5,
  title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={8696--8708},
  year={2021}
}

@inproceedings{CodeBERT,
  title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={1536--1547},
  year={2020}
}

@inproceedings{Transformers,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={6000--6010},
  year={2017}
}
@misc{CodeX,
  doi = {10.48550/ARXIV.2107.03374},
  
  url = {https://arxiv.org/abs/2107.03374},
  
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Evaluating Large Language Models Trained on Code},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Debin,
  title={Debin: Predicting debug information in stripped binaries},
  author={He, Jingxuan and Ivanov, Pesho and Tsankov, Petar and Raychev, Veselin and Vechev, Martin},
  booktitle={Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1667--1680},
  year={2018}
}
@inproceedings{Dire,
  title={DIRE: A neural approach to decompiled identifier naming},
  author={Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
  booktitle={2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={628--639},
  year={2019},
  organization={IEEE}
}
@article{VarBERT,
  title={Variable Name Recovery in Decompiled Binary Code using Constrained Masked Language Modeling},
  author={Banerjee, Pratyay and Pal, Kuntal Kumar and Wang, Fish and Baral, Chitta},
  journal={arXiv preprint arXiv:2103.12801},
  year={2021}
}
@article{SnowWhite,
  title={Finding the Dwarf: Recovering Precise Types from WebAssembly Binaries},
  author={Lehmann, Daniel and Pradel, Michael},
  year={2022}
}

@article{Neutron,
author = {Liang, Ruigang and Cao, Ying and Hu, Peiwei and Chen, Kai},
year = {2021},
month = {03},
pages = {5},
title = {Neutron: an attention-based neural decompiler},
volume = {4},
journal = {Cybersecurity},
doi = {10.1186/s42400-021-00070-0}
}

@inproceedings{StochFuzz,
  title={StochFuzz: Sound and Cost-effective Fuzzing of Stripped Binaries by Incremental and Stochastic Rewriting},
  author={Zhang, Zhuo and You, Wei and Tao, Guanhong and Aafer, Yousra and Liu, Xuwei and Zhang, Xiangyu},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={659--676},
  year={2021},
  organization={IEEE}
}
@inproceedings{ColeOptimizationLevel,
  title={Cole: compiler optimization level exploration},
  author={Hoste, Kenneth and Eeckhout, Lieven},
  booktitle={Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
  pages={165--174},
  year={2008}
}
@article{gccOptimization,
  title={Optimization in GCC},
  author={Jones, M Tim},
  journal={Linux journal},
  volume={2005},
  number={131},
  pages={11},
  year={2005}
}
@article{optimizationObfuscation,
author = {Blazy, Sandrine and Riaud, St\'{e}phanie},
title = {Measuring the Robustness of Source Program Obfuscation: Studying the Impact of Compiler Optimizations on the Obfuscation of C Programs},
year = {2014},
isbn = {9781450322782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2557547.2557577},
doi = {10.1145/2557547.2557577},
abstract = {Obfuscation is a commonly used technique to protect software from the reverse engineering process. Advanced obfuscations usually rely on semantic properties of programs and thus may be performed on source programs. This raises the question of how to be sure that the binary code (that is effectively running) is still obfuscated.This paper presents a data obfuscation of C programs and a methodology to evaluate how the obfuscation resists to the GCC compiler. Information generated by the compiler (including effects of relevant optimizations that could deobfuscate programs) and a study of the disassembled binary code, as well as a dynamic analysis of the performances of binary code show that our obfuscation is worthwhile.},
booktitle = {Proceedings of the 4th ACM Conference on Data and Application Security and Privacy},
pages = {123–126},
numpages = {4},
keywords = {obfuscation, compiler optimization, anti-reverse engineering},
location = {San Antonio, Texas, USA},
series = {CODASPY '14}
}

@inproceedings{reverseEngineerProcess,
author = {Votipka, Daniel and Rabin, Seth and Micinski, Kristopher and Foster, Jeffrey S. and Mazurek, Michelle L.},
title = {An Observational Investigation of Reverse Engineers' Process and Mental Models},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3313040},
doi = {10.1145/3290607.3313040},
abstract = {Reverse engineering is a complex task essential to several software security jobs like vulnerability discovery and malware analysis. While traditional program comprehension tasks (e.g., program maintenance or debugging) have been thoroughly studied, reverse engineering diverges from these tasks as reverse engineers do not have access to developers, source code, comments, or internal documentation. Further, reverse engineers often have to overcome countermeasures employed by the developer to make the task harder (e.g., symbol stripping, packing, obfuscation). Significant research effort has gone into providing program analysis tools to support reverse engineers. However, little work has been done to understand the way they think and analyze programs, potentially leading to the lack of adoption of these tools among practitioners. This paper reports on a first step toward better understanding the reverse engineer's process and mental models and provides directions for improving program analysis tools to better fit their users. We present the initial results of a semi-structured, observational interview study of reverse engineers (n=15). Each interview investigated the questions they asked while probing the program, how they answered these questions, and the decisions made throughout. Our initial observations suggest that reverse engineers rely on a variety of reference points in both the program text and structure as well as its dynamic behavior to build hypotheses about the program's function and identify points of interest for future exploration. In most cases, our reverse engineers used a mix of static and dynamic analysis---mostly manually---to verify these hypotheses. Throughout, they rely on intuition built up over past experience. From these observations, we provide recommendations for user interface and program analysis improvements to support the reverse engineer.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {program comprehension, reverse engineering},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@inproceedings{intermediateTraining,
    title = "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?",
    author = "Pruksachatkun, Yada  and
      Phang, Jason  and
      Liu, Haokun  and
      Htut, Phu Mon  and
      Zhang, Xiaoyi  and
      Pang, Richard Yuanzhe  and
      Vania, Clara  and
      Kann, Katharina  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.467",
    doi = "10.18653/v1/2020.acl-main.467",
    pages = "5231--5247",
}

@article{CodeXGlue,
  doi = {10.48550/ARXIV.2102.04664},
  url = {https://arxiv.org/abs/2102.04664},
  author = {Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and Li, Ge and Zhou, Lidong and Shou, Linjun and Zhou, Long and Tufano, Michele and Gong, Ming and Zhou, Ming and Duan, Nan and Sundaresan, Neel and Deng, Shao Kun and Fu, Shengyu and Liu, Shujie},
  title = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{BERT,
  doi = {10.48550/ARXIV.1810.04805},
  
  url = {https://arxiv.org/abs/1810.04805},
  
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{evaluationSummarization,
  title={On the Evaluation of Neural Code Summarization},
  author={Shi and Ensheng Wang and Yanlin Du and Lun Chen and Junjie Han and Shi Zhang and Hongyu Zhang and Dongmei Sun and Hongbin Sun},
  year={2022},
  publisher={ICSE}
}
@misc{VarCLR,
  doi = {10.48550/ARXIV.2112.02650},
  
  url = {https://arxiv.org/abs/2112.02650},
  
  author = {Chen, Qibin and Lacomis, Jeremy and Schwartz, Edward J. and Neubig, Graham and Vasilescu, Bogdan and Goues, Claire Le},
  
  keywords = {Software Engineering (cs.SE), Computation and Language (cs.CL), Machine Learning (cs.LG), Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{OffensiveLanguage,
  doi = {10.48550/ARXIV.2106.02245},
  
  url = {https://arxiv.org/abs/2106.02245},
  
  author = {Cheriyan, Jithin and Savarimuthu, Bastin Tony Roy and Cranefield, Stephen},
  
  keywords = {Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards offensive language detection and reduction in four Software Engineering communities},
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@article{Nero,
author = {David, Yaniv and Alon, Uri and Yahav, Eran},
title = {Neural Reverse Engineering of Stripped Binaries Using Augmented Control Flow Graphs},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428293},
doi = {10.1145/3428293},
journal = {Proceedings of the ACM on Programming Languages},
month = {nov},
articleno = {225},
numpages = {28},
keywords = {Static Binary Analysis, Neural Reverse Engineering}
}
@inproceedings{PolyglotCodeBERT,
  title={Multilingual training for software engineering},
  author={Ahmed, Toufique and Devanbu, Premkumar},
  booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)},
  pages={1443--1455},
  year={2022},
  organization={IEEE}
}
@inproceedings{FunctionBoundaryDetection,
author = {Alves-Foss, Jim and Song, Jia},
title = {Function Boundary Detection in Stripped Binaries},
year = {2019},
isbn = {9781450376280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359789.3359825},
doi = {10.1145/3359789.3359825},
booktitle = {Proceedings of the 35th Annual Computer Security Applications Conference},
pages = {84–96},
numpages = {13},
keywords = {binary analysis, function boundary detection},
location = {San Juan, Puerto Rico, USA},
series = {ACSAC '19}
}
@misc{ExtremeSummarization,
  doi = {10.48550/ARXIV.1602.03001},
  url = {https://arxiv.org/abs/1602.03001},
  author = {Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Convolutional Attention Network for Extreme Summarization of Source Code},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{Meteor,
  title={The METEOR metric for automatic evaluation of machine translation},
  author={Lavie, Alon and Denkowski, Michael J},
  journal={Machine translation},
  volume={23},
  number={2},
  pages={105--115},
  year={2009},
  publisher={Springer}
}
@inproceedings{Rouge,
  title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@misc{CodeSumSmallLocal,
  doi = {10.48550/ARXIV.2206.00804},
  url = {https://arxiv.org/abs/2206.00804},
  author = {Ahmed, Toufique and Devanbu, Premkumar},
  keywords = {Software Engineering (cs.SE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning code summarization from a small and local dataset},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{FUNCRE,
  title={Learning to Find Usage of Library Functions in Optimized Binaries},
  author={Ahmed, Toufique and Devanbu, Premkumar and Sawant, Anand Ashok},
  journal={IEEE Transactions on Software Engineering},
  year={2021},
  publisher={IEEE}
}
@inbook{CodeSumMetrics,
author = {Roy, Devjeet and Fakhoury, Sarah and Arnaoudova, Venera},
title = {Reassessing Automatic Evaluation Metrics for Code Summarization Tasks},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468588},
pages = {1105–1116},
numpages = {12}
}

@article{seq2seq,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{LSTM,
  title={Long short-term memory},
  author={Schmidhuber, J{\"u}rgen and Hochreiter, Sepp and others},
  journal={Neural Comput},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997}
}

@article{CNN,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}
@article{RNN,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}
@article{GPT3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@incollection{wordnet,
  title={WordNet},
  author={Fellbaum, Christiane},
  booktitle={Theory and applications of ontology: computer applications},
  pages={231--243},
  year={2010},
  publisher={Springer}
}
@inproceedings{dualChannel,
  title={A theory of dual channel constraints},
  author={Casalnuovo, Casey and Barr, Earl T and Dash, Santanu Kumar and Devanbu, Prem and Morgan, Emily},
  booktitle={Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
  pages={25--28},
  year={2020}
}
@article{naturalnessCode,
author = {Hindle, Abram and Barr, Earl T. and Gabel, Mark and Su, Zhendong and Devanbu, Premkumar},
title = {On the Naturalness of Software},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/2902362},
doi = {10.1145/2902362},
journal = {Commun. ACM},
month = {apr},
pages = {122–131},
numpages = {10}
}