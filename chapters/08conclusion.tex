\chapter{Conclusion and Future Work}
\label{conclusion}
% Answer the research questions according to the findings

% \section{Future Work}

% \begin{itemize}
%     \item Exploration of other compiled languages
%     \item Evaluation of different decompilers (Radare/Ida/Hopper)
%     \item Since Ghidra decompiles some functions badly and generally recovers very few additional details, exploration of the application of these methods to disassembled code instead of decompiled code
%     \item Automatic scoring methods to determine the quality of a decompiled sample
%     \item The application of semantic scoring methods to this model
%     \item Exploration of other, weaker strippers
%     \item Developing this model into a Ghidra plugin
%     \item Instead of using base Ghidra, use a different model like DIRE to add identifiers to the stripped code
% \end{itemize}
% \section{Threats to validity}
% \subsection{Internal}
% \begin{itemize}
%     \item Noise, some samples have low-quality comments (Badly written documentation, or incorrectly collected)
%     \item Inductive bias from the pre-trained base model
%     \item A lucky randomly selected test set
%     \item Bias stemming from data collection (only samples that decompile nicely and are commented are represented in the data)
% \end{itemize}
% \subsection{External} 
% \begin{itemize}
%     \item This research only covers unstripped and stripped binaries, some code, particularly malware might use methods to resist analysis
%     \item We use a very bipolar notion of stripping, some binaries might have been stripped with a significantly weaker stripper
    
% \end{itemize}
% \subsection{Construct}
% \begin{itemize}
%     \item We only cover and explore a single state-of-the-art model, other models might be more successful (mono-operation)
%     \item Documentation that can be derived from code, might not be useful at all (mono-method) (can be argued about code summarization in general)
% \end{itemize}
% \section{Ethical Considerations}
% \begin{itemize}
%     \item Automation Bias
%     \item Insulting comments in dataset can reflect in output
%     \item Non-negligible computation costs in training and deduplication, we try to limit this by providing trained models and pre-processed data
%     \item This technique, and reverse engineering techniques in general can be used for malicous purposes. 
% \end{itemize}
% \newpage
To conclude the thesis we will first summarize the answers to the research questions posed in chapter \ref{methodology} based on our findings and discussion. We will then discuss possible directions for future work based on this thesis. 
\section{Summary}
We found that source and unstripped decompiled code performed relatively well, while demi-stripped performed significantly worse. Stripped performed even worse and the resulting model was mostly unusable. We found that duplicates have a relatively large impact on the model performance, removing these duplicates puts the model performance in line with other source-code datasets. This shows that our models are not only reproducing previously encountered summaries, but are also able to create new summaries and have a deeper understanding of the underlying patterns in the data. We found that the loss of identifiers causes a drop in performance and that the introduction of decompilation faults in stripped code has a large impact on the model performance.

We found that the performance of the model can be improved using intermediate-training objectives. Mainly, we found that a Deobfuscation intermediate-training objective improves model performance across the board. We also found that the effectiveness of the intermediate-training objective is dependent on the fine-tuning objective as an objective-mismatch could actually cause a drop in performance.

\section{Future Work}
This work solely focuses on the C language, these same techniques could be applied to other compiled languages. Of particular interest is the Go language, much of the novel malware is written in Go.\\
Besides the exploration of other languages, other stripping techniques could also be explored. We solely focused on the strip tool included in Linux, but other stripping techniques, some of which are less strict than the Linux implementation, could also be explored.\\

Furthermore, we focus solely on the Ghidra decompiler, which is free to use. Other paid decompilers could also be used for the decompilation step. IDA-Pro\footnote{IDA-Pro: \url{https://hex-rays.com/ida-pro/}} features more advanced function identification methods as F.L.I.R.T.\footnote{F.L.I.R.T: \url{https://hex-rays.com/products/ida/tech/flirt/in_depth/}} and Lumina\footnote{Lumina: \url{https://hex-rays.com/products/ida/lumina/}}, which could help with function extraction in stripped binaries.

We found that Ghidra struggles to decompile stripped functions and generally adds very little syntax or identifiers to the function, which is why the exploration of these methods on disassembled code might see more success. Disassembled code should have fewer decompilation errors and mistakes. The use of models like CodeBERT and CodeT5 would probably not see much success however, as these models are trained on source code. Decompiled code is similar to source code by design, but disassembled code has a quite limited but very different vocabulary, which could cause Out-Of-Vocabulary issues. One would therefore need to completely retrain models from scratch on data in the disassembled data.

Another solution would be to keep using these methods, but add an additional module which can determine whether a stripped sample is a well decompiled function or if a decompilation failure has occurred. The improperly decompiled functions could be skipped, and the RE would only receive the high quality output.

The output of Ghidra could also be enhanced using a model like DIRE \cite{Dire} or Debin \cite{Debin} to recover more identifiers, before passing the function to the summarization model. This method would however incur an additional performance penalty since it needs to decompile the binaries and recover the identifiers using one of these models. Furthermore, any mistakes and biases introduced by those models would then also be transferred to our model.


%The use of BLEU-4 as a scoring method also introduces some issues. The currently employed scoring methodology does not take semantics into account, meaning that the model could produce perfectly valid and usable descriptions, but since they do not match the syntax of the baseline, the score will be very low and the model will be penalized. Instead of using BLEU-4, METEOR and ROUGE-L scores to score the candidate against the baseline, a semantic method could also be employed. VarCLR \cite{VarCLR} can determine the semantic similarity of terms using a learned embedding. Note that there is a difference between relatedness, which some other novel solutions employ, and similarity, which VarCLR captures. \textbf{Maximum} and \textbf{Minimum} are highly related but they are not similar, one cannot be substituted with the other while preserving meaning. \textbf{Minimum} and \textbf{Minimal} are both related and similar, since they can be substituted. Unfortunately, VarCLR only scores similarity of variables and not entire sentences.\\


Finally, this work could be developed into a Ghidra plugin. Using Ghidra's scripting engine, it would be relatively straightforward to extract the functions from the decompiled code and to infer the comments using one of our trained models. To prevent the constant loading and unloading of the model in the GPU, and the requirement for a GPU, the models could even be made permanently available online through an API. The script can then insert the comments above the decompiled functions in the decompiler window. The script would then be run when a binary is processed by Ghidra to aid the RE in understanding the binary.
