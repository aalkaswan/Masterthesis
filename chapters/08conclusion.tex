\chapter{Conclusion and Future Work}
\label{conclusion}
% Answer the research questions according to the findings

% \section{Future Work}

% \begin{itemize}
%     \item Exploration of other compiled languages
%     \item Evaluation of different decompilers (Radare/Ida/Hopper)
%     \item Since Ghidra decompiles some functions badly and generally recovers very few additional details, exploration of the application of these methods to disassembled code instead of decompiled code
%     \item Automatic scoring methods to determine the quality of a decompiled sample
%     \item The application of semantic scoring methods to this model
%     \item Exploration of other, weaker strippers
%     \item Developing this model into a Ghidra plugin
%     \item Instead of using base Ghidra, use a different model like DIRE to add identifiers to the stripped code
% \end{itemize}
% \section{Threats to validity}
% \subsection{Internal}
% \begin{itemize}
%     \item Noise, some samples have low-quality comments (Badly written documentation, or incorrectly collected)
%     \item Inductive bias from the pre-trained base model
%     \item A lucky randomly selected test set
%     \item Bias stemming from data collection (only samples that decompile nicely and are commented are represented in the data)
% \end{itemize}
% \subsection{External} 
% \begin{itemize}
%     \item This research only covers unstripped and stripped binaries, some code, particularly malware might use methods to resist analysis
%     \item We use a very bipolar notion of stripping, some binaries might have been stripped with a significantly weaker stripper
    
% \end{itemize}
% \subsection{Construct}
% \begin{itemize}
%     \item We only cover and explore a single state-of-the-art model, other models might be more successful (mono-operation)
%     \item Documentation that can be derived from code, might not be useful at all (mono-method) (can be argued about code summarisation in general)
% \end{itemize}
% \section{Ethical Considerations}
% \begin{itemize}
%     \item Automation Bias
%     \item Insulting comments in dataset can reflect in output
%     \item Non-negligible computation costs in training and deduplication, we try to limit this by providing trained models and pre-processed data
%     \item This technique, and reverse engineering techniques in general can be used for malicous purposes. 
% \end{itemize}
% \newpage
To conclude the thesis, we will first summarize the answers to the research questions posed in chapter \ref{ExperimentalSetup} based on our findings and discussion. We will then discuss possible directions for future work based on this thesis. 
\section{Summary}
We find that source and unstripped decompiled code performed relatively well, while demi-stripped performed significantly worse. Stripped performs even worse, and the resulting model mainly was unusable. We observe that duplicates have a relatively significant impact on the model performance. Removing these duplicates puts the model performance in line with other source-code datasets. This shows that our models are not only reproducing previously encountered summaries but are also able to create new summaries and have a deeper understanding of the underlying patterns in the data. Finally, we discover that the loss of identifiers causes a drop in performance and that the introduction of decompilation faults in stripped code has a large impact on the model performance.

We find that the model's performance can be improved using intermediate-training objectives. Mainly, we observe that a Deobfuscation intermediate-training objective improves model performance across the board. However, we also find that the effectiveness of the intermediate-training objective is dependent on the fine-tuning objective, as an objective mismatch could cause a drop in performance.

\section{Future Work}
This work solely focuses on the C language. These same techniques could be applied to other compiled languages. Of particular interest is the Go language. Much of the novel malware is written in Go.\\
Besides the exploration of other languages, other stripping techniques could also be explored. We solely focused on the stripping tool included in Linux, but other stripping techniques, some of which are less strict than the Linux implementation, could also be explored.\\

Furthermore, we focus solely on the Ghidra decompiler, which is free to use. Other paid decompilers could also be used for the decompilation step. IDA-Pro\footnote{IDA-Pro: \url{https://hex-rays.com/ida-pro/}} features more advanced function identification methods as F.L.I.R.T.\footnote{F.L.I.R.T: \url{https://hex-rays.com/products/ida/tech/flirt/in_depth/}} and Lumina\footnote{Lumina: \url{https://hex-rays.com/products/ida/lumina/}}, which could help with function extraction in stripped binaries.

We found that Ghidra struggles to decompile stripped functions and generally adds minimal syntax or identifiers to the function, which is why exploring these methods on disassembled code might see more success. Disassembled code should have fewer decompilation errors and mistakes. However, using models like CodeBERT and CodeT5 would probably not see much success, as these models are trained on source code. Decompiled code is similar to source code by design, but disassembled code has a relatively limited but very different vocabulary, which could cause Out-Of-Vocabulary issues. One would therefore need to completely retrain models from scratch on data in the disassembled data.

Another solution would be to keep using these methods but add an additional module to determine whether a stripped sample is a well-decompiled function or if a decompilation failure has occurred. Then, the improperly decompiled functions could be skipped, and the RE would only receive the high-quality output.

The output of Ghidra could also be enhanced using a model like DIRE~\cite{Dire} or Debin~\cite{Debin} to recover more identifiers before passing the function to the summarisation model. This method would, however incur an additional performance penalty since it needs to decompile the binaries and recover the identifiers using one of these models. Furthermore, any mistakes and biases introduced by those models would be transferred to our model.


%The use of BLEU-4 as a scoring method also introduces some issues. The currently employed scoring methodology does not take semantics into account, meaning that the model could produce perfectly valid and usable descriptions. However, since they do not match the baseline syntax, the score will be very low, and the model will be penalized. Instead of using BLEU-4, METEOR and ROUGE-L scores to score the candidate against the baseline, a semantic method could also be employed. VarCLR~\cite{VarCLR} can determine the semantic similarity of terms using a learned embedding. Note that there is a difference between relatedness, which some other novel solutions employ, and similarity, which VarCLR captures. \textbf{Maximum} and \textbf{Minimum} are highly related, but they are not similar; one cannot be substituted with the other while preserving meaning. \textbf{Minimum} and \textbf{Minimal} are both related and similar since they can be substituted. Unfortunately, VarCLR only scores similarity of variables and not entire sentences.\\


Finally, this work could be developed into a Ghidra plugin. Using Ghidra's scripting engine, extracting the functions from the decompiled code and infer the comments using one of our trained models would be relatively straightforward. To prevent the constant loading and unloading of the model in the GPU and the requirement for a GPU, the models could even be made permanently available online through an API. The script can then insert the comments above the decompiled functions in the decompiler window. The script would then be run when Ghidra processes a binary to aid the RE in understanding the binary.


