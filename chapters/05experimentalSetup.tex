\chapter{Experimental Setup}
\label{ExperimentalSetup}
% \begin{itemize}
%     \item We aim to answer the following questions:
%     \begin{itemize}
%         \item RQ1: How do different input types (source, unstripped decompiled, demi-stripped, stripped) affect the model's performance (data-richness effect)?
%         \item RQ2: What is the impact of data duplication on the model's performance (data-duplication effect)?
%         \item RQ3: To what extent each aspect of stripped decompiled binaries impacts the model's performance (data-input study)?
%         \item RQ4: How do different pre-training objectives affect the model's performance (model-objective effect)?
%     \end{itemize}
%     \item Explain what script was used for deduplication
%     \item Explain how the model was loaded in and what configurations were used
%     \item Explain which metrics we used to assess the model
%     \item Explain the manual evaluation of the stripped code
%     \item Explain the baseline of CodeT5 on normal programming languages, and our own finetuning on source-C
% \end{itemize}

% \newpage

\section{Research Questions}
We must create and assess our model to reach our goal of creating an automatic system for decompiled code summarisation. We define research questions which we will answer throughout the thesis. Firstly it is essential to assess the different datasets and set a baseline for each. We can then investigate the impact of different aspects of the data. Finally, we can apply this knowledge to design intermediate-training objectives, further improving performance. This leads to the following research questions:

\begin{itemize}
    \item \textbf{RQ1: How do different input types (source, unstripped decompiled, demi-stripped, stripped) affect the model's performance? (data-richness effect)}
    \begin{sloppypar}
    Firstly, we want to know the impact of the data-richness on the model performance. The different datasets have different degrees of data richness. The source code has all of its identifiers and comments in code. Unstripped decompiled code has no comments and loses many of its identifiers. The decompiler also introduces some noise. Demi-stripped data loses all of the remaining identifiers. Stripped data also has no identifiers and introduces even more decompilation noise.
    \end{sloppypar}
    \item \textbf{RQ2: What is the impact of data duplication on the model's performance? (data-duplication effect)}
    \begin{sloppypar}
    Secondly, we will evaluate how the model reacts to data duplication, whether the model performance is simply a result of the memorisation of certain examples, or if the performance results from a generalisable understanding of the data.
    \end{sloppypar}
    \item \textbf{RQ3: To what extent does each aspect of stripped decompiled binaries impact the model's performance? (data-input study)}
    \begin{sloppypar}
    The different datasets each contain different aspects of the original source code. The dataset size, number of decompilation errors and the type and number of recovered identifiers. Which of these aspects are most important for the model performance? 
    \end{sloppypar}
    \item \textbf{RQ4: How do different intermediate-training objectives affect the model's performance? (model-objective effect)}
    \begin{sloppypar}
    Finally, we will apply the insights provided by the previous questions to design new intermediate-training objectives, through which we aim to address the shortcomings of the base model. 
    \end{sloppypar}
\end{itemize}

\section{Dataset}
To answer the research questions, we construct a diverse and representative dataset. The Buildswarm dataset contains around 1.8m aligned decompiled-sourcecode pairs and 400k aligned stripped-sourcecode pairs. The significant difference is caused by the inherent difficulty finding functions in stripped decompiled code. 

From this dataset, we collect any documentation located above the functions using srcML. For example, figure~\ref {fig:srcML} shows an example of the source code of a function from the ECDSA library. Above the function definition (line 401), we locate a function block. 

\begin{figure}[tbh]
  \centering
  \includegraphics[width=\linewidth]{img/srcML.png}
  \caption{Example function with its documentation (truncated)}
  \label{fig:srcML}
\end{figure}

This documentation can be split into the following classes: 
\begin{enumerate}
  \item Double slash comments, example from Jep:release\_utf\_char. 
\begin{minted}{C}
    // release memory allocated by jstring2char
\end{minted}
    These comments are thrown out, as these are generally not used for documentation.
  \item Single line comments, example from Nesbox:Curl\_mime\_read.
\begin{minted}{C}
    /* Set mime part remote file name. */
\end{minted}
   We take the entire comment as a description.
  \item Multiline comments, example from oftc-ircservices:cs\_on\_client\_join.
\begin{minted}[breaklines]{C}
    /**
     * CS Callback when a Client joins a Channel
     * @param args 
     * @return pass_callback(self, struct Client *, char *)
     * When a Client joins a Channel:
     *  - attach DBChannel * to struct Channel*
     */
\end{minted}
    In this case, we take the first line or sentence.
\end{enumerate}



The data is pre-processed using the Pandas \footnote{Pandas: \url{https://pandas.pydata.org/}} data analysis library as well as the Pandarallel \footnote{Pandarallel: \url{https://pypi.org/project/pandarallel/}} parrallelizability extention for Pandas.

The samples are pre-processed by first extracting the function name and adding it as a column to the data. For the stripped samples, we utilise the alignment with C by extracting the function name from the source code for each sample. Next, the samples are aligned with the comment data using the file name where they reside appended with the function name, for instance: \textit{/Repos\_Bionic/ secp256k1/secp256k1\\.c:secp256k1\_pubkey\_load}. We further pre-process the data by removing any newlines from the function body and by unescaping any characters that have been escaped for file safety purposes, such as \&gt; for $>$.

The samples are split into a train, validation and test set. Each set is collected into a single .jsonl \footnote{JSON Lines: \url{https://jsonlines.org/}} file.

\begin{figure}[tbh]
  \centering
\begin{minted}[tabsize=2,breaklines]{json}
[
{"repo":"author/project","docstring_tokens":["Tokenized", "description"], "code_tokens":["Tokenized", "code"], "partition": "train"}
{"repo":"author/project","docstring_tokens":["Tokenized", "description"], "code_tokens":["Tokenized", "code"], "partition": "valid"}
]
\end{minted}
  \caption{Example entries of a .jsonl file with two dummy entries}
  \label{fig:jsonl}
\end{figure}


To answer the second research question, the dataset must be deduplicated. The dataset is deduplicated using a fork\footnote{Near Duplicate Code Detector: \url{https://github.com/SERG-Delft/near-duplicate-code-remover}} of the near-duplicate-code-detector~\cite{allamanis_adverse}. We use this tool to compare all the datasets' functions and find clusters of near-duplicate functions. We randomly select one function per cluster and discard the rest from the dataset. We use the standard tool configuration as recommended by~\citeauthor{allamanis_adverse}. Of the removed duplicates, we observe that a relatively large number originates from common libraries, such as SQLite\footnote{SQLite: \url{https://www.sqlite.org/index.html}}, that are packaged with binary programs. Thus a certain amount of duplication is also likely to occur ``in the wild".

\section{Model Configuration}
To first establish a performance baseline, we train a CodeT5-base model on the summarisation task on source C. The baseline is used to compare the decompiled C, stripped decompiled C and the demi-stripped datasets to the source code. For deduplicated code, the results can be compared with the performance of CodeT5-base on the code summarisation task~\cite{CodeT5}, where the performance varies between 15.24 and 26.03 BLEU4 scores depending on the language. Based on this, we set the baseline for a usable summarisation model around a BLEU4 score of 15, with anything lower than a BLEU4 score of 10 essentially unusable. Besides the standard BLEU and EM metrics used by most code summarisation works~\cite{CodeBERT, CodeT5, CodeXGlue, PolyglotCodeBERT, CodeSumSmallLocal, recommend_summarization}, we also include the  METEOR and ROUGE-L metrics. But we found that the metrics mostly aligned, we therefore mainly focus our discussions on the BLEU-4 score. 

\begin{table}[tbh]
\centering
\begin{tabular}{ll}
\hline
Package        & Version     \\ \hline
Nvidia drivers & 510.60.02   \\
cuda           & 11.6        \\
numpy          & 1.22.2      \\
tensorboard    & 2.8.0       \\
torch          & 1.9.0+cu111 \\
transformers   & 4.16.2      \\
tree-sitter    & 0.20.0      \\ 
Ghidra         & 10.0.4      \tablefootnote{It is not recommended to use Ghidra versions before 10.1 since these versions have not been patched against a Log4J RCE}\\ \hline
\end{tabular}
\caption{The most important packages and their versions}
\label{tab:packages}
\end{table}

A grid search of the optimal settings was infeasible from a time perspective, so we performed training mainly using the recommended settings from the CodeT5-base model~\cite{CodeT5}. We double the source length for the decompiled, stripped, and demi-stripped code to 512 tokens instead of the standard 256 tokens used for the source code. We compensate for the fact that the average length of decompiled code is almost twice as long as the source code. We utilised two different servers for training~\ref{tab:server}. We trained the model on either an NVIDIA RTX3080 with 10GB of VRAM or an NVIDIA GTX 1080ti with 11GB of VRAM. The authors of CodeT5 used an NVIDIA A100 GPU with 40G of VRAM for fine-tuning~\cite{CodeT5}. To compensate for the lack of memory, we reduced the batch size to 2, which was the maximum length that still fit both GPUs \ref{tab:modelSettings}.

\begin{table}[tbh]
\centering
\begin{tabular}{l|ll}
\hline
                & CodeT5-base & Our Settings            \\ \hline
Source length   & 256 Tokens  & \textbf{256/512 Tokens} \\
Target length   & 128 Tokens  & 128 Tokens              \\
Max epochs      & 15          & 15                      \\
Patience        & 2           & 2                       \\
Batch Size      & 32          & \textbf{2}              \\
Vocabulary Size & 32100       & 32100                  
\end{tabular}
\caption{Model configuration of the base model and our used settings}
\label{tab:modelSettings}
\end{table}

\begin{table}[tbh]
\centering
\begin{tabular}{l|ll}
\hline
        & Server 1           & Server 2                     \\ \hline
CPU     & Intel XEON E5-2620 & AMD Ryzen Threadripper 3990X \\
Cores   & 16 (32 threads)    & 64 (128 threads)             \\
RAM     & 192GB              & 128GB                        \\
GPU     & Nvidia GTX 1080TI  & Nvidia RTX 3080              \\
VRAM    & 11GB               & 10GB                         \\
Storage & 7.3TB HDD          & 1TB NVME SSD                
\end{tabular}
\caption{Hardware used for training and evaluation}
\label{tab:server}
\end{table}

\section{Manual Evaluation}
To investigate the third research question, namely the data-input study, we compare the results of the different datasets to see the influence of the different aspects of code on the model performance. Furthermore, we observe that there could be two principal reasons for a sample to be malformed. Firstly, Ghidra can fail to decompile the function correctly. Secondly, during the data collection phase, the comment might not have been appropriately parsed, which results in an incorrect description of the function. To investigate this influence on the stripped model's performance, we randomly sample 25 high and low-performing samples (in terms of BLEU-4 score) and manually analyse the decompiled code and the description. 

\section{Intermediate-Training}

We constructed three intermediate-training objectives, namely Translation, Deobfuscation and Span Prediction, to address the model-objective research question.

\subsection{Translation}
The first defined intermediate-training task is a Neural Machine Translation task. In this code-to-code task, the model has to translate the source code from one programming language to another~\cite{CodeXGlue}. 
We implement a translation from demi-stripped to unstripped decompiled code in our case. Note that, by construction, the only difference between decompiled and demi-stripped code is the lack of identifiers in the demi-stripped code. Figure~\ref{fig:tanslation} shows an example of a training sample. The input field denotes the input to the model, and the output field depicts the expected model output. We task the model with "translating" a decompiled function back to the source code. Note that besides the missing identifiers, the structure is also slightly different.

\begin{figure}[tbh]
  \centering
\begin{minted}{C}
repo: NLnetLabs/ldns
input: "undefined8 MASK0 ( long param_1 ) { 
    undefined8 uVar1; 
    if (param_1 != 0) { 
        uVar1 = MASK1(); 
        uVar1 = MASK2(uVar1); 
        return uVar1; 
    } 
    return 0; 
}"
target: "uint8_t ldns_rr_label_count (const ldns_rr * rr ){ 
    if (!rr) { 
        return 0;
    } 
    return ldns_dname_label_count (ldns_rr_owner(rr));
}"
\end{minted}
  \caption{Translation intermediate training objective}
  \label{fig:tanslation}
\end{figure}

\subsection{Deobfuscation}
The second defined task is a deobfuscation objective. In this code-to-text objective, we task the model with predicting the identifiers in demi-stripped code. Recall that in the demi-stripped code, all identifiers are masked with meaningless placeholders, where duplicate identifiers are assigned the same placeholder. The model will have to output a map of the placeholders to their original value. While the model's output is somewhat textual and not code, it is not precisely natural language. Figure~\ref{fig:dobf} shows an example of a training sample. 
 
\begin{figure}[tbh]
  \centering
\begin{minted}{C}
repo: NLnetLabs/ldns
input: "undefined8 MASK0 ( long param_1 ) { 
    undefined8 uVar1; 
    if (param_1 != 0) { 
        uVar1 = MASK1(); 
        uVar1 = MASK2(uVar1); 
        return uVar1; 
    } 
    return 0; 
}"
target: "{MASK0: ldns_rr_label_count, MASK1:
ldns_rr_owner, MASK2: ldns_dname_label_count}"
\end{minted}
  \caption{Deofbuscation intermediate training objective}
  \label{fig:dobf}
\end{figure}


\subsection{Span Prediction}
Finally, we define a Span Prediction objective. In this code-to-text objective, we task the model with recovering the identifiers from demi-stripped code. Although, unlike the DOBF objective, every identifier (even matching identifiers) is assigned unique placeholders, the model has to output the assignment of the placeholders in a form closer to natural language and puts more emphasis on duplicated identifiers which might be more critical. Figure~\ref{fig:spanDetection} depicts an illustration of a training sample. 

\begin{figure}[tbh]
  \centering
\begin{minted}{C}
repo: NLnetLabs/ldns
input: "undefined8 MASK0 ( long param_1 ) { 
    undefined8 uVar1; 
    if (param_1 != 0) { 
        uVar1 = MASK1(); 
        uVar1 = MASK2(uVar1); 
        return uVar1; 
    } 
    return 0; 
}"
target: "MASK0 ldns_rr_label_count MASK1
ldns_rr_owner MASK2 ldns_dname_label_count"
\end{minted}
  \caption{Span-detection intermediate training objective}
  \label{fig:spanDetection}
\end{figure}

