\chapter{Experimental Setup}
\label{ExperimentalSetup}
% \begin{itemize}
%     \item We aim to answer the following questions:
%     \begin{itemize}
%         \item RQ1: How do different input types (source, unstripped decompiled, demi-stripped, stripped) affect the model's performance (data-richness effect)?
%         \item RQ2: What is the impact of data duplication on the model's performance (data-duplication effect)?
%         \item RQ3: To what extent each aspect of stripped decompiled binaries impacts the model's performance (data-input study)?
%         \item RQ4: How do different pre-training objectives affect the model's performance (model-objective effect)?
%     \end{itemize}
%     \item Explain what script was used for deduplication
%     \item Explain how the model was loaded in and what configurations were used
%     \item Explain which metrics we used to assess the model
%     \item Explain the manual evaluation of the stripped code
%     \item Explain the baseline of CodeT5 on normal programming languages, and our own finetuning on source-C
% \end{itemize}

% \newpage

\section{Research Questions}
To create and assess our model and contributions, we define the following research questions which will be answered throughout the thesis.
\todo[inline]{you can first add a bit of story here to connect the RQs together, then go to the details of each question}

\begin{itemize}
    \item \textbf{RQ1: How do different input types (source, unstripped decompiled, demi-stripped, stripped) affect the model's performance? (data-richness effect)}
    \begin{sloppypar}
    Firstly, we want to know how the impact of the data-richness on the model performance. The different datasets that we have different degrees of data richness, the source-code has all of its identifiers and has comments in the code. Unstripped decompiled code has no comments and loses many of its identifiers, some noise is also introduced by the decompiler. Demi-stripped data loses all of the remaining identifiers. Stripped data, also has no identifiers and introduces even more decompilation noise.
    \end{sloppypar}
    \item \textbf{RQ2: What is the impact of data duplication on the model's performance? (data-duplication effect)}
    \begin{sloppypar}
    Secondly we will evaluate how the model reacts to data duplication, whether the model performance is simply a result of the memorization of certain examples, or if the performance is a result of a generalizable understanding of the data.
    \end{sloppypar}
    \item \textbf{RQ3: To what extent each aspect of stripped decompiled binaries impacts the model's performance? (data-input study)}
    \begin{sloppypar}
    The different datasets each contain different aspect of the original source code, which of these aspects are most important towards the model performance? 
    \end{sloppypar}
    \item \textbf{RQ4: How do different pre-training objectives affect the model's performance? (model-objective effect)}
    \begin{sloppypar}
    Finally, we will apply the insights provided by the previous questions to design new pre-training objectives, through which we aim to address the shortcomings of the base-model. 
    \end{sloppypar}
\end{itemize}

\section{Dataset}
To answer the research questions a diverse and representative dataset must be constructed. The Buildswarm dataset contains around 1.8m aligned decompiled-sourcecode pairs as well as 400k aligned stripped-sourcecode pairs. The large difference is caused by the inherent difficulty of finding functions in stripped decompiled code. 

From this dataset we collect any documentation that is located above the functions using srcML \ref{fig:srcML}. 
\label{fig:srcML}
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{img/srcML.png}
  \caption{Example function with its documentation (truncated)}
\end{figure}

This documentation can be split into the following classes: 
\begin{enumerate}
  \item Double slash comments, example from Jep:release\_utf\_char. 
\begin{verbatim}
    // release memory allocated by jstring2char
\end{verbatim}
    These comments are thrown out, as these are generally not used for documentation.
  \item Single line comments, example from Nesbox:Curl\_mime\_read.
\begin{verbatim}
    /* Set mime part remote file name. */
\end{verbatim}
   We take the entire comment as a description.
  \item Multiline comments, example from oftc-ircservices:cs\_on\_client\_join.
\begin{verbatim}
    /**
     * CS Callback when a Client joins a Channel
     * @param args 
     * @return pass_callback(self, struct Client *, char *)
     * When a Client joins a Channel:
     *  - attach DBChannel * to struct Channel*
     */
\end{verbatim}
    In this case we take the first line or sentence.
\end{enumerate}



The data is pre-processed using the Pandas \footnote{Pandas: \url{https://pandas.pydata.org/}} data analysis library as well as the Pandarallel \footnote{Pandarallel: \url{https://pypi.org/project/pandarallel/}} parrallelizability extention for Pandas.

The samples are pre-processed by first extracting the function name and adding it as a column to the data, for the stripped samples the allignment with C is utilized by extracting the function name from the sourcecode for each of the samples. The samples are aligned with the comment data using the name of the file where they reside appended with the function name, for instance: \textit{/Repos\_Bionic/ secp256k1/secp256k1.c :secp256k1\_pubkey\_load}. We further pre-process the data by removing any newlines from the function body and by unescaping any characters that have been escaped for file safety purposes such as \&gt; for $>$.

The samples are split into a train, validation and test set. Each set is collected into a single .jsonl \footnote{JSON Lines: \url{https://jsonlines.org/}} file.

\label{fig:jsonl}
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{img/jsonl.png}
  \caption{Example entries of a .jsonl file used to train/evaluate code summarization from stripped code, for brevity the function body has been truncated.}
\end{figure}

\todo[inline]{you can avid the header here. also you can move all the libraries and tools that you used to the configuration/implementation details subsection (under experimental setting). note that you can explain the general process here, but it is better to move the details of implementation under one subsection (configuration/implementation). also make sure that you do not repeat content from data set collection in the "methodology" chapter.}


To answer the second research question the dataset must be deduplicated. The dataset is deduplicated using a fork\footnote{Near Duplicate Code Detector: \url{https://github.com/SERG-Delft/near-duplicate-code-remover}} of the near-duplicate-code-detector \cite{allamanis_adverse}. We use this tool to compare all the functions in the dataset and to find clusters of near-duplicate functions. We randomly select one function per cluster and discard the rest from the dataset. 

% Slightly expand this 

\section{Model Configuration}
To first establish a performance baseline we trained a CodeT5-base model on the summarization task on the source C. This can be used to compare the decompiled C, stripped decompiled C and the demi-stripped datasets to source code. For deduplicated code, the results can be compared with the performance of CodeT5-base on the code summarization task \ref{tab:codeT5Baseline}, where the performance varies between 15.24 and 26.03 BLEU4 score depending on the language \ref{tab:codeT5Baseline}. Based on this, and table \ref{tab:BLEUScale} we set the baseline for a usable summarization model around a BLEU4 score of 15, with anything lower than a BLEU4 score of 10, essentially unusable.


We also trained the model on the deduplicated datasets. 
\label{tab:codeT5Baseline}
\begin{table}[H]
\centering
\begin{tabular}{|l|llllll}
\hline
\rowcolor[HTML]{C0C0C0} 
Model                 & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Ruby} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Javascript} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Go} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Python} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Java} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}PHP} \\ \hline
Seq2Seq               & 9.64                                              & 10.21                                                   & 13.98                                           & 15.93                                               & 15.09                                             & 21.08                                            \\ \cline{1-1}
Transformer           & 11.18                                             & 11.59                                                   & 16.38                                           & 15.81                                               & 16.26                                             & 22.12                                            \\ \cline{1-1}
RoBERTa               & 11.17                                             & 11.90                                                   & 17.72                                           & 18.14                                               & 16.47                                             & 24.02                                            \\ \cline{1-1}
CodeBERT              & 12.16                                             & 14.90                                                   & 18.07                                           & 19.06                                               & 17.65                                             & 25.16                                            \\ \cline{1-1}
PLBART                & 14.11                                             & 15.56                                                   & 18.91                                           & 19.30                                               & 18.45                                             & 23.58                                            \\ \cline{1-1}
CodeT5-small          & 14.87                                             & 15.32                                                   & 19.25                                           & 20.04                                               & 19.92                                             & 25.46                                            \\ \cline{1-1}
CodeT5-base           & 15.24                                             & 16.16                                                   & 19.56                                           & 20.01                                               & 20.31                                             & 26.03                                            \\ \cline{1-1}
\end{tabular}
\caption{BLEU-4 performance of CodeT5 compared to other models on the code summarization task using the CodeXGlue dataset \cite{CodeT5}}
\end{table}

\label{tab:packages}
\begin{table}[!h]
\centering
\begin{tabular}{ll}
\hline
Package        & Version     \\ \hline
Nvidia drivers & 510.60.02   \\
cuda           & 11.6        \\
numpy          & 1.22.2      \\
tensorboard    & 2.8.0       \\
torch          & 1.9.0+cu111 \\
transformers   & 4.16.2      \\
tree-sitter    & 0.20.0      \\ 
Ghidra         & 10.0.4      \tablefootnote{It is not recommended to use Ghidra versions before 10.1 since these versions have not been patched against a Log4J RCE}\\ \hline
\end{tabular}
\caption{The most important packages and their versions}
\end{table}

A grid search of the optimal settings was infeasible from a time perspective, so training was performed using mostly the recommended settings. For the decompiled, stripped, and demi-stripped the source length was doubled to 512 tokens instead of the standard 256 tokens used for the source code. This was done to compensate for the fact that the average length of decompiled and stripped functions was almost double that of the source code. We utilized two different servers for training\ref{tab:server}. Training was performed on either an NVIDIA RTX3080 with 10GB of VRAM or an NVIDIA GTX 1080ti with 11GB of VRAM. The authors of CodeT5 used an NVIDIA A100 GPUs with 40G of VRAM for fine-tuning \cite{CodeT5}. To compensate for the lack of memory we reduced the batchsize to 2, which was the maximum length that still fit both GPUs \ref{tab:modelSettings}.

\label{tab:modelSettings}
\begin{table}[!h]
\centering
\begin{tabular}{l|ll}
\hline
                & CodeT5-base & Our Settings            \\ \hline
Source length   & 256 Tokens  & \textbf{256/512 Tokens} \\
Target length   & 128 Tokens  & 128 Tokens              \\
Max epochs      & 15          & 15                      \\
Patience        & 2           & 2                       \\
Batch Size      & 32          & \textbf{2}              \\
Vocabulary Size & 32100       & 32100                  
\end{tabular}
\caption{Model configuration of the base model and our used settings}
\end{table}

\label{tab:server}
\begin{table}[!h]
\centering
\begin{tabular}{l|ll}
\hline
        & Server 1           & Server 2                     \\ \hline
CPU     & Intel XEON E5-2620 & AMD Ryzen Threadripper 3990X \\
Cores   & 16 (32 threads)    & 64 (128 threads)             \\
RAM     & 192GB              & 128GB                        \\
GPU     & Nvidia GTX 1080TI  & Nvidia RTX 3080              \\
VRAM    & 11GB               & 10GB                         \\
Storage & 7.3TB HDD          & 1TB NVME SSD                
\end{tabular}
\caption{Hardware used for training and evaluation}
\end{table}

\section{Manual Evaluation}
To investigate the third research question, the results of the different datasets are compared, to see the influence of the different aspects of code on the model performance. Furthermore, we observe that there could be two principal reasons for a sample to be malformed. Firstly, Ghidra can fail to properly decompile the function. Secondly, during the data collection phase the comment might not have been parsed properly, which results in an incorrect description of the function. To investigate the influence of this on the performance of the stripped model, we randomly sample 25 high and low performing samples (in terms of BLEU-4 score) and manually analyse the decompiled code and the description. 

\section{Intermediate-Training}

To answer the fourth and final research question, we constructed three intermediate-training objectives.

\subsection{Translation}
The first defined intermediate-training task, is a Neural Machine Translation task. In this code-to-code task, the model has to translate the source code from one programming-language to another \cite{CodeXGlue}. 
In our case we implemented a translation from demi-stripped to unstripped decompiled code. Note that, by construction, the only difference between decompiled and demi-stripped code, is the lack of identifiers in the demi-stripped code.

\label{fig:tanslation}
\begin{figure}[H]
  \centering
\begin{lstlisting}
repo: NLnetLabs\ldns
input: "undefined8 MASK0 ( long param_1 ) { 
    undefined8 uVar1; 
    if (param_1 != 0) { 
        uVar1 = MASK1(); 
        uVar1 = MASK2(uVar1); 
        return uVar1; 
    } 
    return 0; 
}"
target: "uint8_t ldns_rr_label_count (const ldns_rr * rr ){ 
    if (!rr) { 
        return 0;
    } 
    return ldns_dname_label_count (ldns_rr_owner(rr));
}"
\end{lstlisting}
  \caption{Translation intermediate training objective}
\end{figure}

\subsection{Deobfuscation}
The second defined task, is a deobfuscation objective. In this code-to-text objective, the model is tasked with predicting the identifiers in demi-stripped code. Recall, that in the demi-stripped code, all identifiers are masked with meaningless placeholders, where duplicate identifiers are assigned the same placeholder. The model will have to output a map of the placeholders to their original value. While the output of the model is somewhat textual and not code, it is not exactly natural language.

\label{fig:dobf}
\begin{figure}[H]
  \centering
\begin{lstlisting}
repo: NLnetLabs\ldns
input: "undefined8 MASK0 ( long param_1 ) { 
    undefined8 uVar1; 
    if (param_1 != 0) { 
        uVar1 = MASK1(); 
        uVar1 = MASK2(uVar1); 
        return uVar1; 
    } 
    return 0; 
}"
target: "{MASK0: ldns_rr_label_count, MASK1:
ldns_rr_owner, MASK2: ldns_dname_label_count}"
\end{lstlisting}
  \caption{Deofbuscation intermediate training objective}
\end{figure}


\subsection{Span Prediction}
Finally, we define a Span Prediction objective. In this code-to-text objective, the model is tasked with recovering the identifiers from demi-stripped code. Unlike the DOBF objective, every identifier (even matching identifiers) are assigned unique placeholders, the model has to output the assignment of the placeholders in a form which is closer to natural language and puts more emphasis on duplicated identifiers which might be more important.

\label{fig:spanDetection}
\begin{figure}[H]
  \centering
\begin{lstlisting}
repo: NLnetLabs\ldns
input: "undefined8 MASK0 ( long param_1 ) { 
    undefined8 uVar1; 
    if (param_1 != 0) { 
        uVar1 = MASK1(); 
        uVar1 = MASK2(uVar1); 
        return uVar1; 
    } 
    return 0; 
}"
target: "MASK0 ldns_rr_label_count MASK1
ldns_rr_owner MASK2 ldns_dname_label_count"
\end{lstlisting}
  \caption{Span-detection intermediate training objective}
\end{figure}