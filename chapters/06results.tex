\chapter{Results}
\label{results}
% Present the following findings from the experiments:

% \begin{itemize}
%     \item RQ1:
%     \begin{itemize}
%         \item From the dataset, we collect relatively few stripped functions compared to the decompiled code
%         \item A pre-trained CodeT5 Model performs well on Source, Decompiled and Demi-stripped code, but relatively poor on stripped code
%         \item The model performance is not correlated with compiler optimization level
%     \end{itemize}

%     \item RQ2:
%         \begin{itemize}
%         \item We lose a large number of samples through deduplication
%         \item Duplicates have a relatively large influence on performance, removing them puts the performance of source code and decompiled code in line with normal languages
%         \item Duplicates usually originate from external libraries which are packaged with the binary 
%     \end{itemize}
%     \item RQ3:
%     \begin{itemize}
%         \item The performance on stripped code is significantly lower than the demi-stripped data
%         \item The manual evaluation shows that higher-quality stripped data correlates with a higher score
%     \end{itemize}
%     \item RQ4:
%         \begin{itemize}
%         \item Using the DOBF pre-training objective, we were able to increase the model's performance on the stripped and demi-stripped data. The other objectives did not perform better than the base model
%         \end{itemize}
% \end{itemize}
% \newpage
In this chapter, the results of the experiments are presented, and the results are grouped by research question.

\section{RQ1: Data-richness study}
We collect around 40k stripped-description from the dataset and around 480k decompiled-description and C-description pairs. 

The performance of the base model on each of the datasets is presented in the following table:

\begin{table}[!h]
\centering
\begin{tabular}{lllll} 
\hline
\rowcolor[rgb]{0.761,0.761,0.761} \multicolumn{1}{|l}{\textbf{Duplicated}} & BLEU-4 & EM    & METEOR & \multicolumn{1}{l|}{ROUGE-L}  \\ 
\hline
C                                                                          & 36.97  & 25.56 & 38.34  &  40.92                            \\
DecomC                                                                     & 33.26  & 20.20 & 34.65  & 37.79                             \\
Demi                                                                       & 21.69  & 13.10 & 21.22  &  23.33                             \\
Stripped                                                                   & 9.53   & 3.41  & 8.53  &  10.43                          
\end{tabular}
\caption{Result of fine-tuning CodeT5-base on the different datasets}
\label{tab:duplicated}
\end{table}

We found that the C and unstripped decompiled models generally produced good summaries. The summaries produced by the demi-stripped model were significantly worse, but most were still very usable. The stripped model mainly produced summaries which were unusable. Most sequences produced by the model were grammatically and syntactically correct and had some meaning. These could have easily passed for a summary but not for the targeted function.

\begin{table}[!h]
\centering
\begin{tabular}{ll}
\hline
\rowcolor[HTML]{9B9B9B} 
\multicolumn{1}{|l}{\cellcolor[HTML]{9B9B9B}Reference} & \multicolumn{1}{l|}{\cellcolor[HTML]{9B9B9B}Model Output}    \\ \hline
Destroy getline object and nullify its pointer.               & Unload a C library.
\end{tabular}
\caption{Reference and output of fine-tuned stripped model, note that while the output reads like natural language, it is completely meaningless and incorrect in the context of this function.}
\label{tab:syntax}
\end{table}

Furthermore, we observed some examples where the model produced a relatively good summary which would likely be very useful but differed heavily from the ground truth. This caused the output to be scored poorly, and the model would suffer a penalty during training.

\begin{table}[!h]
\centering
\begin{tabular}{ll}
\hline
\rowcolor[HTML]{9B9B9B} 
\multicolumn{1}{|l}{\cellcolor[HTML]{9B9B9B}Reference} & \multicolumn{1}{l|}{\cellcolor[HTML]{9B9B9B}Model Output} \\ \hline
Sort variables by type                                 & qsort an array of values by type.       
\end{tabular}
\caption{Reference and output of fine-tuned decompiled model, note that the model output is more descriptive and accurate than the reference}
\label{tab:betterOutput}
\end{table}


\section{RQ2: Data-duplication study}
After deduplication, we are left with around 218k decompiled-description pairs and only 7.5k stripped-description pairs. The performance of the base model on each of the datasets is presented in the following table:

\begin{table}[!h]
\centering
\begin{tabular}{lllll} 
\hline
\rowcolor[rgb]{0.749,0.749,0.749} \multicolumn{1}{|l}{\textbf{Deduplicated}} & BLEU-4 & EM    & METEOR & \multicolumn{1}{l|}{ROUGE-L}  \\ 
\hline
C                                                                            & 28.17  & 14.82 & 30.6  &  33.51                             \\
DecomC                                                                       & 19.09  & 4.68  & 21.12  &  24.55                             \\
Demi                                                                         & 11.52  & 2.24  & 11.31  &  13.81
                             \\
Stripped                                                                     & 7.03   & 0.91  & 6.15  &  7.49                            
\end{tabular}
\caption{Result of fine-tuning CodeT5-base on the deduplicated datasets}
\label{tab:deduplicated}
\end{table}
We find that the influence of deduplication on the base model's performance is relatively small on source code While having a relatively large impact on the decompiled and demi-stripped code. 
Of the removed duplicates, we find that a relatively large number originates from common libraries that are packaged with binaries. 

%maybe add example

\section{RQ3: Data-input study}
We find that the performance on decompiled code is slightly lower than source code \ref{tab:duplicated}, while the performance of demi-stripped code is much higher than stripped code. To control for the impact of the dataset size, we reduce the size of the dataset of the demi-stripped code to match the stripped dataset and fine-tune a CodeT5-base model using the same setup.

\begin{table}[!h]
\centering
\begin{tabular}{llllll} 
\hline
\rowcolor[rgb]{0.729,0.729,0.729} \multicolumn{1}{|l}{\textbf{}} & Samples & BLEU-4 & EM   & METEOR & \multicolumn{1}{l|}{ROUGE-L}  \\ 
\hline
Demi                                                             & 40k     & 17.48  & 8.22 & 17.22  &  19.68                             \\
Stripped                                                         & 40k     & 9.53   & 3.41 & 6.6  &  7.61                            
\end{tabular}
\caption{Comparison between a reduced Demi-stripped and Stripped CodeT5-base model}
\label{tab:demiSize}
\end{table}

We find that the dataset size does not sufficiently explain the significant difference between the demi-stripped and stripped performance. To further investigate this, high and low performing stripped samples were investigated. We randomly select 25 samples above and 25 below a certain BLEU-4 score threshold. We set this threshold at a BLEU-4 score of 50.

\begin{table}[!h]
\centering
\begin{tabular}{lll}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|l}{\cellcolor[HTML]{C0C0C0}\textbf{}} & Decompilation Failure & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Bad Description} \\ \hline
High                                                   & 4/25                     & 6/25                                                            \\
Low                                                    & \textbf{8/25}                     & 7/25                                                           
\end{tabular}
\caption{Number of samples which were badly decompiled and which had a badly mined description respectively}
\label{tab:manual}
\end{table}

We find that the number of inaccurate descriptions is similar between the high and low-scoring samples. However, on the flip side, the number of decompilation failures is much higher.
Furthermore, we found that all the decompiled generally had very few recovered symbols, making them very syntactically poor compared to actual programming languages.

%enter example here

\section{RQ4: Model-objective study}
\begin{table}[!h]
\centering
\begin{tabular}{l|llll|llll}
\rowcolor[rgb]{0.749,0.749,0.749} \multicolumn{1}{l}{} & \multicolumn{4}{l|}{Stripped}                                                                                                                        & Demi                               &                                  &                                      &                                       \\ 
\cline{2-9}
\begin{sideways}\textbf{}\end{sideways}                & \begin{sideways}BLEU4\end{sideways} & \begin{sideways}EM\end{sideways} & \begin{sideways}METEOR\end{sideways} & \begin{sideways}ROUGE\_L\end{sideways} & \begin{sideways}BLEU4\end{sideways} & \begin{sideways}EM\end{sideways} & \begin{sideways}METEOR\end{sideways} & \begin{sideways}ROUGE\_L\end{sideways}  \\ 
\hline
Baseline                                               & 9.53                                & 3.41                          & 8.53                                & 10.43                                   & 21.69                              & 13.10  & 21.22                                & 23.33                                                           \\
TRANS                                                  & 5.22                                & 0.92     & 4.81  &  5.44                                                       & 20.74                              & 11.72                               & 20.3  &  22.37                               \\
\textbf{DOBF}                                          & \textbf{9.98}                       & \textbf{3.49}                    & \textbf{8.81}                       & \textbf{11.17}                       & \textbf{23.23}                     & \textbf{12.35}                   & \textbf{22.77}                       & \textbf{24.93}                        \\
SPAN                                                   & 9.42                                & 3.15                             & 8.64  &  10.68                                & 22.18                              & 11.47                            & 21.19                                &   24.45                              
\end{tabular}
\caption{Result of fine-tuning CodeT5-base after intermediate-training}
\label{tab:intermediate}
\end{table}
The translation intermediate-training objective did not yield higher scores in either stripped or demi-stripped code. We found that the deobfuscation objective resulted in significantly higher scores in both stripped and demi-stripped code. The span prediction intermediate-training objective did not yield any improvement for the stripped code but slightly improved the performance on demi-stripped code compared to the baseline.

