\chapter{Results}
\label{results}
% Present the following findings from the experiments:

% \begin{itemize}
%     \item RQ1:
%     \begin{itemize}
%         \item From the dataset, we collect relatively few stripped functions compared to the decompiled code
%         \item A pre-trained CodeT5 Model performs well on Source, Decompiled and Demi-stripped code, but relatively poor on stripped code
%         \item The model performance is not correlated with compiler optimization level
%     \end{itemize}

%     \item RQ2:
%         \begin{itemize}
%         \item We lose a large number of samples through deduplication
%         \item Duplicates have a relatively large influence on performance, removing them puts the performance of source code and decompiled code in line with normal languages
%         \item Duplicates usually originate from external libraries which are packaged with the binary 
%     \end{itemize}
%     \item RQ3:
%     \begin{itemize}
%         \item The performance on stripped code is significantly lower than the demi-stripped data
%         \item The manual evaluation shows that higher-quality stripped data correlates with a higher score
%     \end{itemize}
%     \item RQ4:
%         \begin{itemize}
%         \item Using the DOBF pre-training objective, we were able to increase the model's performance on the stripped and demi-stripped data. The other objectives did not perform better than the base model
%         \end{itemize}
% \end{itemize}
% \newpage
In this chapter, the results of the experiments are presented, and the results are grouped by research question.

\section{RQ1: Data-richness study}
We collect around 40k stripped-description pairs from the dataset and around 480k decompiled-description and C-description pairs. 

The performance of the CodeT5-base model on each of the datasets is presented in table~\ref{tab:deduplicated}. The metrics are calculated for each individual sample from the test set, and the average score is presented in the table.

\begin{table}[tbh]
\centering
\begin{tabular}{lcccc} 
\hline
\rowcolor[rgb]{0.761,0.761,0.761} \multicolumn{1}{|l}{\textbf{}} & BLEU-4 & EM    & METEOR & \multicolumn{1}{l|}{ROUGE-L}  \\ 
\hline
C                                                                          & 36.97  & 25.56 & 38.34  &  40.92                            \\
DecomC                                                                     & 33.26  & 20.20 & 34.65  & 37.79                             \\
Demi                                                                       & 21.69  & 13.10 & 21.22  &  23.33                             \\
Stripped                                                                   & 9.53   & 3.41  & 8.53  &  10.43                          
\end{tabular}
\caption{Result of fine-tuning CodeT5-base on the different datasets}
\label{tab:duplicated}
\end{table}

We found that the C and DecomC models generally produced good summaries, evident by the BLEU-4 scores over 30. The summaries produced by the demi-stripped model were substantially worse, but most were still very usable, evident by the BLEU-4 score above 20. The stripped model mainly produced unusable summaries, as evidenced by the BLEU-4 score below 10. However, most sequences produced by the model were grammatically and syntactically correct and had some meaning. These could have easily passed for a summary but not for the targeted function. For example, the model produced the output ``Unload a C library" against the reference ``Destroy getline object and nullify its pointer", this output is completely meaningless in the context of the targeted function.

Furthermore, we observed some examples where the model produced a relatively good summary which would likely be very useful but differed heavily from the ground truth. This caused the output to be scored poorly, and the model would suffer a penalty during training. For example, the model produced the output ``qsort an array of values by type" against the reference ``Sort variables by type", which results in a low BLEU-4 score, despite being more descriptive.

\section{RQ2: Data-duplication study}
After deduplication, we are left with around 218k decompiled-description pairs and only 7.5k stripped-description pairs. The performance of the base model on each of the datasets is presented in table~\ref{tab:deduplicated}:
\begin{table}[htb]
\centering
\begin{tabular}{lcccc|c} 
\hline
\rowcolor[rgb]{0.729,0.729,0.729} \multicolumn{1}{|l}{\textbf{Deduplicated}} & BLEU-4 & EM    & METEOR & ROUGE-L & \multicolumn{1}{c|}{$\Delta$BLEU-4}  \\ 
\hline
C                                                                            & 28.17  & 14.82 & 30.60   & 33.51   & 8.80                                 \\
DecomC                                                                       & 19.09  & 4.68  & 21.12  & 24.55   & 14.17                                \\
Demi                                                                         & 11.52  & 2.24  & 11.31  & 13.81   & 10.17                                \\
Stripped                                                                     & 7.03   & 0.91  & 6.15   & 7.49    & 2.50                                
\end{tabular}
\caption{Result of fine-tuning CodeT5-base on the deduplicated datasets and the difference with the baseline}
\label{tab:deduplicated}
\end{table}

We find that the influence of deduplication on the our model's performance is relatively small on source code, of only 14\%. Duplicate have a relatively large impact on the decompiled (43\%) and demi-stripped (47\%) code. 
Of the removed duplicates, we observe that a relatively large number originates from common libraries that are packaged with binary programs. 

%maybe add example

\section{RQ3: Data-input study}
We find that the performance on decompiled code is slightly lower than source code \ref{tab:duplicated}, while the performance of demi-stripped code is much higher than stripped code. To control for the impact of the dataset size, we reduce the size of the dataset of the demi-stripped code to match the stripped dataset and fine-tune a CodeT5-base model using the same setup.

\begin{table}[tbh]
\centering
\begin{tabular}{llcccc} 
\hline
\rowcolor[rgb]{0.729,0.729,0.729} \multicolumn{1}{|l}{\textbf{}} & Samples & BLEU-4 & EM   & METEOR & \multicolumn{1}{l|}{ROUGE-L}  \\ 
\hline
Demi                                                             & 40k     & 17.48  & 8.22 & 17.22  &  19.68                             \\
Stripped                                                         & 40k     & 9.53   & 3.41 & 6.6  &  7.61                            
\end{tabular}
\caption{Comparison between a reduced Demi-stripped and Stripped CodeT5-base model}
\label{tab:demiSize}
\end{table}

We find that the dataset size does not sufficiently explain the significant difference between the demi-stripped and stripped performance. To further investigate this, high and low performing stripped samples were investigated. We randomly select 25 samples above and 25 below a certain BLEU-4 score threshold. We set this threshold at a BLEU-4 score of 50.

\begin{table}[tbh]
\centering
\begin{tabular}{lll}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|l}{\cellcolor[HTML]{C0C0C0}\textbf{}} & Decompilation Failure & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Bad Description} \\ \hline
High                                                   & 4/25                     & 6/25                                                            \\
Low                                                    & \textbf{8/25}                     & 7/25                                                           
\end{tabular}
\caption{Number of samples which were badly decompiled and which had a badly mined description respectively}
\label{tab:manual}
\end{table}

We find that the number of inaccurate descriptions is similar (6 vs 7) between the high and low-scoring samples. However, on the flip side, the number of decompilation failures is much higher (4 vs 8).
Furthermore, we found that all the decompiled code generally had very few recovered symbols, making it very syntactically poor compared to actual programming languages.

%enter example here

\section{RQ4: Model-objective study}
The results of the intermediate-training objectives are presented in table~\ref{tab:intermediate}. Since the source code and unstripped code perform well, we only focus on improving the performance of the stripped and demi-stripped code. The intermediate-training objectives are baselined against the results from RQ1.

\begin{table}[tbh]
\centering
\begin{tabular}{l|llll|llll}
\rowcolor[rgb]{0.749,0.749,0.749} \multicolumn{1}{l}{} & \multicolumn{4}{l|}{Stripped}                                                                                                                        & Demi                               &                                  &                                      &                                       \\ 
\cline{2-9}
\begin{sideways}\textbf{}\end{sideways}                & \begin{sideways}BLEU4\end{sideways} & \begin{sideways}EM\end{sideways} & \begin{sideways}METEOR\end{sideways} & \begin{sideways}ROUGE\_L\end{sideways} & \begin{sideways}BLEU4\end{sideways} & \begin{sideways}EM\end{sideways} & \begin{sideways}METEOR\end{sideways} & \begin{sideways}ROUGE\_L\end{sideways}  \\ 
\hline
Baseline                                               & 9.53                                & 3.41                          & 8.53                                & 10.43                                   & 21.69                              & 13.10  & 21.22                                & 23.33                                                           \\
TRANS                                                  & 5.22                                & 0.92     & 4.81  &  5.44                                                       & 20.74                              & 11.72                               & 20.30  &  22.37                               \\
\textbf{DOBF}                                          & \textbf{9.98}                       & \textbf{3.49}                    & \textbf{8.81}                       & \textbf{11.17}                       & \textbf{23.23}                     & \textbf{12.35}                   & \textbf{22.77}                       & \textbf{24.93}                        \\
SPAN                                                   & 9.42                                & 3.15                             & 8.64  &  10.68                                & 22.18                              & 11.47                            & 21.19                                &   24.45                              
\end{tabular}
\caption{Result of fine-tuning CodeT5-base after intermediate-training}
\label{tab:intermediate}
\end{table}
Adding the translation intermediate-training objective did not yield higher scores in either stripped or demi-stripped code across all metrics. We found that the deobfuscation objective resulted in substantially higher scores in both stripped and demi-stripped code across the metrics. The only exception being the EM rate. The span prediction intermediate-training objective yielded mixed results, only improving the METEOR and ROUGE-L scores for stripped code and BLEU-4 and ROUGE-L for the demi-stripped code.
