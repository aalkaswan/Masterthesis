\chapter{Results}
\label{results}
% Present the following findings from the experiments:

% \begin{itemize}
%     \item RQ1:
%     \begin{itemize}
%         \item From the dataset, we collect relatively few stripped functions compared to the decompiled code
%         \item A pre-trained CodeT5 Model performs well on Source, Decompiled and Demi-stripped code, but relatively poor on stripped code
%         \item The model performance is not correlated with compiler optimization level
%     \end{itemize}

%     \item RQ2:
%         \begin{itemize}
%         \item We lose a large number of samples through deduplication
%         \item Duplicates have a relatively large influence on performance, removing them puts the performance of source code and decompiled code in line with normal languages
%         \item Duplicates usually originate from external libraries which are packaged with the binary 
%     \end{itemize}
%     \item RQ3:
%     \begin{itemize}
%         \item The performance on stripped code is significantly lower than the demi-stripped data
%         \item The manual evaluation shows that higher-quality stripped data correlates with a higher score
%     \end{itemize}
%     \item RQ4:
%         \begin{itemize}
%         \item Using the DOBF pre-training objective we were able to increase the performance of the model on the stripped data and demi-stripped data, the other objectives did not perform better than the base model
%         \end{itemize}
% \end{itemize}
% \newpage

In this chapter the results of the experiments are presented, the results are grouped by research question.

\todo[inline]{in this chapter you can include sample results, sample summaries generated by the model. if you wan tot discuss them extensively, you can also move it to the discussion chapter}

\section{Data-richness}
From the dataset we collect around 40k stripped-description, and around 480k decompiled-description and C-description pairs. 

The performance of the base model on each of the datasets is presented in the following table:
\todo[inline]{you can move the demi to third row and stripped to the final row of the following table.  it will keep the flow of results and also is consistent with the order you discuss results in the next paragraph}
\label{tab:duplicated}
\begin{table}[!h]
\centering
\begin{tabular}{lll}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|l}{\cellcolor[HTML]{C0C0C0}\textbf{Duplicated}} & BLEU-4  & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}EM} \\ \hline
C                                                                  & 36.97 & 25.56                                           \\
DecomC                                                             & 33.26 & 20.20                                           \\
Demi                                                               & 21.69 & 13.10   
                            \\
Stripped                                                           & 9.53  & 3.41                \end{tabular}
\caption{Result of fine-tuning CodeT5-base on the different datasets}
\end{table}

We found that the C and unstripped decompiled models generally produced good summaries. The summaries produced by the demi-stripped model where significantly worse, but most were still very usable. The stripped model mostly produced summaries which were unusable. Most sequences produced by the model were grammatically and syntactically correct and had some meaning. These could have easily passed for a summary, but not for the targeted function.

\label{tab:syntax}
\begin{table}[!h]
\centering
\begin{tabular}{ll}
\hline
\rowcolor[HTML]{9B9B9B} 
\multicolumn{1}{|l}{\cellcolor[HTML]{9B9B9B}Reference} & \multicolumn{1}{l|}{\cellcolor[HTML]{9B9B9B}Model Output}    \\ \hline
Destroy getline object and nullify its pointer.               & Unload a C library.
\end{tabular}
\caption{Reference and output of fine-tuned stripped model, note that while the output reads like natural language, it is completely meaningless and incorrect}
\end{table}

Furthermore, we found some examples where the model produced a relatively good summary which would likely be very useful, but differed heavily from the ground truth. This caused the output to be scored poorly and the model would suffer a penalty during training.
\label{tab:betterOutput}
\begin{table}[!h]
\centering
\begin{tabular}{ll}
\hline
\rowcolor[HTML]{9B9B9B} 
\multicolumn{1}{|l}{\cellcolor[HTML]{9B9B9B}Reference} & \multicolumn{1}{l|}{\cellcolor[HTML]{9B9B9B}Model Output} \\ \hline
Sort variables by type                                 & \textbf{qsort an array of values by type.}        
\end{tabular}
\caption{Reference and output of fine-tuned decompiled model, note that the model output is more descriptive and accurate than the reference}
\end{table}


\section{Data-duplication}
After deduplication we are left with around 218k decompiled-description pairs and 33k stripped-description pairs. The performance of the base model on each of the datasets is presented in the next table:

\label{tab:deduplicated}
\begin{table}[!h]
\centering
\begin{tabular}{lll}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|l}{\cellcolor[HTML]{C0C0C0}\textbf{Deduplicated}} & BLEU-4  & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}EM} \\ \hline
C                                                                  & 28.17 & 14.82                                           \\
DecomC                                                             & 19.09 & 4.68                                            \\
Demi                                                           & XXX   & XXX                                             \\
Stripped                                                               & XXX   & XXX                                            
\end{tabular}
\caption{Result of fine-tuning CodeT5-base on the deduplicated dataset}
\end{table}
We find that the influence of deduplication on the performance of the base model is relatively small on source code, While having a relatively large impact on the decompiled and demi-stripped code. 
Of the removed duplicates, we find that a relatively large number originates from common libraries that are packaged with the binaries. 

%maybe add example

\section{Data-input}
We find that the performance on decompiled code is slightly lower than source code \ref{tab:duplicated}, while the performance of demi-stripped code is much higher than stripped code. To control for the impact of the dataset size, we reduce the size of the dataset of the demi-stripped code to match the stripped dataset and fine-tune a CodeT5-base model using the same setup.
\label{tab:demiSize}
\begin{table}[!h]
\centering
\begin{tabular}{llll}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|l}{\cellcolor[HTML]{C0C0C0}\textbf{}} & Samples & BLEU-4 & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}EM} \\ \hline
Demi                                                   & 40k     & 17.48  & 8.22
                                       \\
Stripped                                               & 40k     & 9.53   & 3.41                                           
\end{tabular}
\caption{Comparison between a reduced Demi-stripped and Stripped CodeT5-base model}
\end{table}

We find that the dataset size does not sufficiently explain the large difference between the demi-stripped and stripped performance. To further investigate this, high and low performing stripped samples were investigated. We randomly select 25 samples above and 25 samples below a certain BLEU-4 score threshold. We set this threshold at a BLEU-4 score of 50.

\label{tab:manual}
\begin{table}[!h]
\centering
\begin{tabular}{lll}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|l}{\cellcolor[HTML]{C0C0C0}\textbf{}} & Decompilation Failure & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Bad Description} \\ \hline
High                                                   & 8/25                     & 7/25                                                            \\
Low                                                    & 4/25                     & 6/25                                                           
\end{tabular}
\caption{Number of samples which were badly decompiled and which had a badly mined description respectively}
\end{table}

We find that the number of bad descriptions is similar between the high and low scoring samples. On the flip side, the number of decompilation failures is much higher.
Furthermore, we found that all the decompiled generally had very few recovered symbols, making it very syntactically poor compared to actual programming languages.

%enter example here

\section{Model-objective}

\label{tab:intermediate}
\begin{table}[!h]
\centering
\begin{tabular}{lll|ll}
\rowcolor[HTML]{C0C0C0} 
                               & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}Stripped} & Demi           &       \\ \cline{2-5} 
\multicolumn{1}{l|}{\textbf{}} & BLEU-4                    & EM                        & BLEU           & EM    \\ \hline
\multicolumn{1}{l|}{Baseline}  & 9.53                      & 3.41                      & 21.69          & 13.10 \\
\multicolumn{1}{l|}{TRANS}     & 5.22                      & 0.92                      & 20.74          & 11.72 \\
\multicolumn{1}{l|}{DOBF}      & \textbf{9.98}             & \textbf{3.49}             & \textbf{23.23} & \textbf{12.35}   \\
\multicolumn{1}{l|}{SPAN}      & 9.42                      & 3.15                      & 22.18          & 11.47
\end{tabular}
\caption{Result of fine-tuning CodeT5-base after intermediate-training}
\end{table}

The translation intermediate-training objective did not yield higher scores in neither stripped or demi-stripped code. We found that the deobfuscation objective resulted in significantly higher scores in both stripped and demi-stripped code. The span prediction intermediate-training objective did not yield any improvement for the stripped code, but did slightly improve the performance on demi-stripped code compared to the baseline.