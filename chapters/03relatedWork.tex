\chapter{Related Work}
\label{relatedWork}
% \begin{description}
%     \item[Debin] Debin use a statistical model to predict variable names from stripped assembly. But suffers from poor generalisability and poor performance on other datasets \cite{VarBERT}[p.1]
%     \item[Dire] Dire use an LSTM and A GNN to predict variable names and types from stripped binaries.
%     \item[VarBERT] VarBERT uses a BERT model, pre-trained with a constrained MLM objective, to predict variable names and types
%     \item[Neutron] Neutron apply Machine Translation to translate from decompiled (not stripped) code back to source code using an LSTM  
%     \item[FUNCRE] InlinedFunc apply pre-training and fine tuning to a RoBERTa model to identify and recover usages of inlined functions in decompiled code
%     \item[SnowWhite] Recently, SnowWhite used a statistical model to predict types from webassembly binaries
%     \item[StochFuzz] Recently, StochFuzz applies the use of fuzzers, to analyse stripped binaries for vulnerabilities, but the fuzzer only finds vulnerable inputs and does not give any insight into the binary itself
% \end{description}
In this section, we provide an overview of studies most related to our goal of generating summaries for binary functions. Binary reverse engineering and the use of NLP for software engineering are vast and active fields, so we select and discuss the closest state-of-the-art works in the field. We categorise the studies into three groups: identifier recovery, binary translation and code summarisation. Finally, we will discuss the open challenges and the relation of our own work to these challenges. 

\section{Recovering Identifiers from Stripped Binaries}
\textbf{Debin} \cite{Debin} aims to recover debug information from stripped binaries. The authors use a tree-based classification model to determine which registers in the intermediate representation are useful to predict and map to an actual variable, as opposed to constant values and intermediate values, which have no mapping to source code variables. A probabilistic graph-based model is built, and all the variable names and types are jointly recovered using a maximum a posteriori probability inference. The authors show state-of-the-art performance in the recovery of both names and types. Notably, the authors claim no noticeable performance penalty between different optimisation levels.\footnote{Debin Presentation at CCS' 18: \url{https://youtu.be/x1x_KtS-5Hs?t=1551}} 

Unlike previous works, \textbf{VarBERT} \cite{VarBERT} uses a Transformer-based NLP model for the task of variable name recovery. The authors pre-trained a BERT model with a standard Masked Language Modelling (MLM) objective. This same model is then fine-tuned to predict the names and types from unstripped binaries using a constrained Masked Language Modelling objective. The authors show state-of-the-art performance in the recovery of names and types. Furthermore, the authors of VarBERT have shown that the performance of Debin does not generalise well to other datasets \cite{VarBERT}. 

\textbf{FUNCRE} \cite{FUNCRE} uses a pre-trained and fine-tuned ROBERTA model to predict usages of inlined library functions. Recall that compilers with optimisations enabled can inline functions in the binary (Chapter~\ref{background}). Therefore, identifying library function calls is key to understanding the behaviour of the binary. The authors use indelible markers, which do not get destroyed by compiler optimisations, to mark usages of library functions. The model is pre-trained with an MLM objective on the stripped decompiled code. It is then fine-tuned to detect and name inlined function usages in a given context window. The authors combine their model with the function recovery functionality included in Ghidra. The resulting combined model significantly improved the performance of Ghidra's inlined function recovery capabilities. The performance exceeded Hex-Ray's IDA-pro, which performs significantly better than Ghidra at inlined function recovery \cite{FUNCRE}. 

\section{Binary Translation}
\textbf{Neutron} \cite{Neutron} frames decompilation as a neural machine translation problem and utilises an LSTM-based neural translation network to translate disassembled binaries back to C source code. The binaries are not stripped and do not have any optimisations enabled. To handle long-term information, the LSTM also includes an attention mechanism. The authors first remove the identifiers before translation, ensuring the identifiers are aligned with the translated identifier-less source code. The translations created by Neutron can contain syntax errors, so the authors apply regular expressions to create a tailor-made syntax checker. Neutron achieves high accuracy on the translation task, but only on unstripped and non-optimised code.

\section{Automatic Source Code Summarisation}
\textbf{PolyglotCodeBERT} \cite{PolyglotCodeBERT} applies multilingual training to a CodeBERT \cite{CodeBERT} model. The use of multilingual and multimodal pre-training objectives is a  well-established paradigm; recall the use of multilingual and bimodal pre-training objectives in CodeT5 \cite{CodeT5} (Chapter~\ref{background}). \citeauthor{PolyglotCodeBERT} propose a multilingual fine-tuning objective to augment the labelled fine-tuning dataset, to improve the performance of languages with less abundant labelled data. The authors found that identifiers are an essential aspect of training data and that identifiers are not language-specific. Recall that unstripped decompiled binaries have few identifiers, and that stripped decompiled binaries have no identifiers. We will, therefore further investigate the importance of identifiers in decompiled binaries

\citeauthor{PolyglotCodeBERT} use the CodeXGLUE dataset to train their model. The multilingual aspect is achieved by mixing samples from different programming languages in training set to train the model in all languages simultaneously. The single multilingual model is then tested on the test set of the different programming languages in the CodeXGLUE dataset. Using this approach the authors were able to report the highest-scoring model on the CodeXGLUE \cite{CodeXGlue} \footnote{CodeXGLUE benchmark: \url{https://microsoft.github.io/CodeXGLUE/}} Code Summarisation benchmark. Their score was later surpassed by CodeT5 \cite{CodeT5}. PolyglotCodeBERT reported an average BLEU4 score of 20.06 over all the programming languages, with the greatest improvements reported in Ruby and JavaScript, the languages with the fewest samples in CodeXGLUE. Recently, DistillCodeT5 reported an even higher score than PolyglotCodeBERT and CodeT5. Unfortunately, as of the writing of this work, DistillCodeT5 is not yet published, and we were unable to find any details about the implementation.

\begin{table}[!h]
\begin{sideways}
\begin{tabulary}{1.5\textwidth}{lLLLLL}
\rowcolor[HTML]{C0C0C0} 
Work     & Task                            & Approach                                           & Source                         & Level             & Limitations               \\
Debin    & Variable name and type recovery & Extremely Randomized Trees and MAP graphs           & Stripped binary                & Binary level      & Poor generalisability     \\
VarBERT  & Variable name and type recovery & Pre-trained and fine-tuned BERT model              & Stripped binary                & Function level    &                           \\
FUNCRE   & Inlined function recovery       & Pre-trained and fine-tuned RoBERTa model           & Stripped binary                & Function level    &                           \\
Neutron  & Neural Machine Translation              & Attention-LSTM                                     & Unstripped binary              & Instruction level & No compiler optimisations \\ 
PolyglotCodeBERT & Code summarisation       & Fine-tuned CodeBERT model   & Source Code & Function level    &   \\
Our work & Binary Code summarisation              & Intermediate-trained and fine-tuned CodeT5 model   & Stripped and unstripped decompiled binary & Function level    &   \\
\end{tabulary}
\end{sideways}
\caption{Overview of related works and our proposed solution.}
\label{tab:related}
\end{table}

\subsection{Open Challenges}
Although there have been many studies on binary identifier recovery and code summarisation, several aspects have not been properly addressed and investigated. The application of code summarisation methods to decompiled code, has not been addressed by any work at all \ref{tab:related}. Neutron~\cite{Neutron} proposes a Neural Machine Translation solution but does not consider compiler optimisations. Debin~\cite{Debin} and VarBERT recover variable/function names and types. PolyglotCodeBERT proposes a state-of-the-art code summarisation method but only focuses on source code \ref{tab:related}. Furthermore, some works on binary code fail to take compiler optimisations or stripping into account~\cite{Neutron}. We, therefore, investigate the application of code summarisation methods to decompiled stripped and unstripped code. Furthermore, we enable compiler optimisations.
