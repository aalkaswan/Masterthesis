\chapter{Related Work}
\label{relatedWork}
% \begin{description}
%     \item[Debin] Debin use a statistical model to predict variable names from stripped assembly. But suffers from poor generalizability and poor performance on other datasets \cite{VarBERT}[p.1]
%     \item[Dire] Dire use an LSTM and A GNN to predict variable names and types from stripped binaries.
%     \item[VarBERT] VarBERT uses a BERT model, pre-trained with a constrained MLM objective, to predict variable names and types
%     \item[Neutron] Neutron apply Machnine Translation to translate from decompiled (not stripped) code back to source code using an LSTM  
%     \item[FUNCRE] InlinedFunc apply pre-training and fine tuning to a RoBERTa model to identify and recover usages of inlined functions in decompiled code
%     \item[SnowWhite] Recently, SnowWhite used a statistical model to predict types from webassembly binaries
%     \item[StochFuzz] Recently, StochFuzz applies the use of fuzzers, to analyse stripped binaries for vulnerabilities, but the fuzzer only finds vulnerable inputs and does not give any insight into the binary itself
% \end{description}
In this section, we provide an overview of the related studies. 
Binary reverse engineering and the use of NLP for software engineering are very large and active fields so we select and discuss the closest SOTA works in the field. We catagorize the studies in three groups namely, identifier recovery, binary translation and code summarization. Finally we will discuss the open challenges and the relation of our own work to these challenges. 

\section{Recovering Identifiers from Stripped Binaries}
\textbf{Debin} \cite{Debin} aims to recover debug information from stripped binaries. The authors use a tree-based classification model to determine which registers in the IR are useful to predict and map to an actual variable, as opposed to constant values and intermediate values which have no mapping to source code variables. A probabilistic graph-based model is built and all the variable names and types are jointly recovered using a Maximum a Posterior probability inference. The authors show SOTA performance in the recovery of both names and types. Notably, the authors claim no noticeable performance penalty between different optimization levels. \footnote{Debin Presentation at CCS' 18: \url{https://youtu.be/x1x_KtS-5Hs?t=1551}} 

Unlike previous works, \textbf{VarBERT} \cite{VarBERT} uses a Transformer-based NLP model for the task of variable name recovery. The authors pre-trained a BERT model with a standard Masked Language Modeling (MLM) objective. This same model is then fine-tuned to predict the names and types from unstripped binaries using a constrained Masked Language Modeling objective. The authors show SOTA performance in the recovery of names and types. Furthermore, the authors of VarBERT have shown that the performance of Debin does not generalize well to other datasets \cite{VarBERT}[p.1]. 

\textbf{FUNCRE} \cite{FUNCRE} uses a pre-trained and fine-tuned ROBERTA model to predict usages of inlined library functions. Recall that compilers with optimizations enabled can inline functions in the binary \ref{background}, identifying library function calls is key to understanding the behaviour of the binary. The authors use indelible markers, which do not get destroyed by compiler optimizations, to mark usages of library functions. The model is pre-trained with an MLM objective on the stripped decompiled code. And then fine-tuned to detect and name inlined function usages in a given context window. The authors combine their model with the function recovery functionality included in Ghidra. The resulting combined model and greatly improved the performance of Ghidra's inlined function recovery capabilities. The performance exceeded Hex-Ray's IDA-pro which performs significantly better than Ghidra at inlined function recovery \cite{FUNCRE}. 

\section{Binary Translation}
\textbf{Neutron} \cite{Neutron} frames decompilation as a neural machine translation problem and utilizes an LSTM-based neural translation network to translate disassembled binaries back to C source code. The binaries are not stripped and do not have any optimizations enabled. To handle long-term information, the LSTM also includes an attention mechanism. The authors first remove the identifiers before translation, the identifiers are aligned with the translated identifier-less source code. The translations created by Neutron can contain syntax errors, so the authors apply regular expressions to create a tailor-made syntax checker. Neutron achieves high accuracy on the translation task, but only on unstripped and non-optimized code.

\section{Automatic Source Code Summarization}
\textbf{PolyglotCodeBERT} \cite{PolyglotCodeBERT} applies multilingual training to a CodeBERT \cite{CodeBERT} model. The use of multilingual and multimodal pre-training objectives is a  well-established paradigm, recall the use of multilingual and bimodal pre-training objectives in CodeT5 \cite{CodeT5} \ref{background}. \citeauthor{PolyglotCodeBERT} propose a multilingual fine-tuning objective to augment the labelled fine-tuning dataset, to improve the performance of languages with less abundant labelled data. The authors found that identifiers are a very important aspect of training data and that identifiers are not language-specific. This conclusion would pose a worrying challenge to our proposed model since decompiled code does not have a lot of identifiers. 

\citeauthor{PolyglotCodeBERT} use the CodeXGLUE dataset to train their model. The multilingual aspect is achieved by mixing the samples from different programming languages in the training set such that the model is trained on all languages simultaneously. The single multilingual model is then tested on the test set of the different programming languages in the CodeXGLUE dataset. Using this approach the authors were able to report the highest-scoring model on the CodeXGLUE \cite{CodeXGlue} \footnote{CodeXGLUE benchmark: \url{https://microsoft.github.io/CodeXGLUE/}} Code Summarization benchmark. Their score was later surpassed by CodeT5 \cite{CodeT5}. PolyglotCodeBERT reported an average BLEU4 score of 20.06 over all the programming languages, with the greatest improvements reported in Ruby and JavaScript, the languages with the fewest samples in CodeXGLUE. Recently, DistillCodeT5 reported an even higher score than PolyglotCodeBERT and CodeT5. Unfortunately as of the writing of this work, DistillCodeT5 is not yet published, and we were unable to find any details about the implementation.

\label{tab:related}
\begin{table}[!h]
\begin{sideways}
\begin{tabulary}{1.5\textwidth}{lLLLLL}
\rowcolor[HTML]{C0C0C0} 
Work     & Task                            & Approach                                           & Source                         & Level             & Limitations               \\
Debin    & Variable name and type recovery & Extremely Randomized Trees and MAP graphs           & Stripped binary                & Binary level      & Poor generalizability     \\
VarBERT  & Variable name and type recovery & Pre-trained and fine-tuned BERT model              & Stripped binary                & Function level    &                           \\
FUNCRE   & Inlined function recovery       & Pre-trained and fine-tuned RoBERTa model           & Stripped binary                & Function level    &                           \\
Neutron  & Neural Translation              & Attention-LSTM                                     & Unstripped binary              & Instruction level & No compiler optimizations \\ 
PolyglotCodeBERT & Code summarization       & Fine-tuned CodeBERT model   & Source Code & Function level    &   \\
Our work & Binary Code summarization              & Intermediate-trained and fine-tuned CodeT5 model   & Stripped and unstripped binary & Function level    &   \\

\end{tabulary}
\end{sideways}
\end{table}

\subsection{Open Challenges}
Although there have been many studies on binary identifier recovery and code summarization, several aspects have not been properly addressed and investigated. The application of code summarization methods to decompiled code, has not been addressed by any work at all \ref{tab:related}. Neutron \cite{Neutron} proposes a Neural Machine Translation solution but does not take compiler optimizations into account. Debin \cite{Debin} and VarBERT recover variable/function names and types. PolyglotCodeBERT proposes a SOTA code summarization method but only focuses on source code \ref{tab:related}. Furthermore, some works on binary code, fail to take compiler optimizations or stripping into account \cite{Neutron}. We therefore investigate the application of code summarization methods to decompiled stripped and unstripped code, furthermore we enable compiler optimizations.
